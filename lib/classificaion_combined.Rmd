---
title: "R Notebook"
output: html_notebook
---

 
```{r}
packages.used=c("gbm", "caret","DMwR" ,"nnet","randomForest","e1071")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
library(gbm)
library(caret)
library(nnet)
library(randomForest)
library(e1071)
```

# Load and Process Data 
```{r}
setwd("~/Spr2017-proj5-grp15/data/observation/")
short_after_selected <- read.csv("short_after20.csv")
long_after_selected <- read.csv("long_before20.csv")

colnames(long_after_selected)[1] <- "y"
long_after_selected$y <- ifelse(long_after_selected$RETURN >0, 1, 0)
long_after_selected <- long_after_selected[,c(-2,-12)]

colnames(short_after_selected)[1] <- "y"
short_after_selected$y <- ifelse(short_after_selected$RETURN >0, 1,0)
short_after_selected <- short_after_selected[,c(-2,-12)]

test.index <- sample(1:1500,300,replace = F)

test.sas <- short_after_selected[test.index,]
test.lbs <- long_after_selected[test.index,]
test.sas.x <- test.sas[,-1]
test.lbs.x <- test.lbs[,-1]
train.sas <- short_after_selected[-test.index,]
train.lbs <- long_after_selected[-test.index,]


```
```{r}
source("../lib/evaluation_measures.R")
```


##### Short After Model 
# GBM
```{r}
# GBM
res_gbm = gbm(y~.,data=train.sas,dist="adaboost",n.tree = 400,shrinkage=0.1)
pred.gbm = predict(res_gbm, test.sas.x, n.tree = 400, type = 'response')
pred.gbm = ifelse(pred.gbm > 0.5, 1, 0)
table(pred.gbm,test.sas$y)

```
# SVM
```{r}
# model.svm <- svm(y ~ ., data = train.sas, cost = 256, gamma = 0.3)
# Tune svm
train.sas$y <- as.factor(train.sas$y)

start.time <- Sys.time()
svm_tune <- tune(svm, 
                 y~DY+EBITG+EV2EBITDA+M2B+MOMENTUM+PB+PE+PF+PS+days+surprise,
                 data=train.sas,
                 kernel="radial", 
                 ranges=list(cost=2^(3:8), 
                             gamma=c(0.3,0.4,0.5)))

model.svm.sas <- svm_tune$best.model
pre.svm <- predict(model.svm,test.sas.x,type="class")
svm.sas <- table(pre.svm,test.sas$y)
performance_statistics(svm.sas)
end.time <- Sys.time()
svm.sas.time <- end.time-start.time
```


# BPNN
```{r}
# netural network
start.time <- Sys.time()
# model.nnet <- nnet(y ~ ., data = train.sas, linout = F, size = 10, decay = 0.001, maxit = 200, trace = F, MaxNWts=6000)
# Tune bpnn
tune_bpnn <- tune.nnet(y ~ ., data = train.sas, linout = F,
          size = 1:10, decay = 0.001:0.1, maxit = 200,
          trace = F, MaxNWts=6000)
model.nnet <- tune_bpnn$best.model
pre.nnet <- predict(model.nnet,test.sas.x,type="class")
nnet.sas <- table(pre.nnet,test.sas$y)
performance_statistics(nnet.sas)
end.time <- Sys.time()
nnet.sas.time <- end.time-start.time

```


# Random Forest 
```{r}
# Random Forest
start.time <- Sys.time()
y.index<- which(colnames(train.sas)=="y")
bestmtry <- tuneRF(y= train.sas$y, x= train.sas[,-y.index], stepFactor=1.5, improve=1e-5, ntree=600)
best.mtry <- bestmtry[,1][which.min(bestmtry[,2])]
model.rf <- randomForest(y ~ ., data = train.sas, ntree=600, mtry=best.mtry, importance=T)
pre.rf <- predict(model.rf,test.sas.x,type="class")
rf.sas <- table(pre.rf,test.sas$y)
performance_statistics(rf.sas)
end.time <- Sys.time()
rf.sas.time <- end.time-start.time
```



# Logistic
```{r}
res_logi = glm(y~.,family = binomial(link="logit"),data=train.sas)
pred.logi = predict(res_logi, test.sas.x, type = 'response')
pred.logi = ifelse(pred.logi > 0.5, 1, 0)
table(pred.logi,test.sas$y)

```



# Majority Vote(Equal Weight)
```{r}
# Majority Vote
pre=(as.numeric(as.character(pre.svm))+as.numeric(as.character(pred.gbm))+as.numeric(as.character(pre.rf)))
pre=ifelse(pre>=2,1,0)
table(pre,test.sas$y)

```
###### Long - Before Model 

# SVM
```{r}
# model.svm <- svm(y ~ ., data = train.lbs, cost = 256, gamma = 0.3)
# Tune svm
train.lbs$y <- as.factor(train.lbs$y)
start.time <- Sys.time()
svm_tune <- tune(svm, 
                 y~DY+EBITG+EV2EBITDA+M2B+MOMENTUM+PB+PE+PF+PS+days,
                 data=train.lbs,
                 kernel="radial", 
                 ranges=list(cost=2^(-1:9), 
                             gamma=c(0.2,0.3,0.4,0.5,0.6)))

model.svm.lbs <- svm_tune$best.model
pre.svm <- predict(model.svm.lbs,test.lbs.x,type="class")
svm.lbs <- table(pre.svm,test.lbs$y)
performance_statistics(svm.lbs)
end.time <- Sys.time()
svm.time <- end.time-start.time
```

# BPNN
```{r}
# netural network
start.time <- Sys.time()
# model.nnet <- nnet(y ~ ., data = train.sas, linout = F, size = 10, decay = 0.001, maxit = 200, trace = F, MaxNWts=6000)
# Tune bpnn
tune_bpnn <- tune.nnet(y ~ ., data = train.lbs, linout = F,
          size = 1:10, decay = 0.001:0.1, maxit = 200,
          trace = F, MaxNWts=6000)
model.nnet <- tune_bpnn$best.model
pre.nnet <- predict(model.nnet,test.lbs.x,type="class")
nnet.lbs <- table(pre.nnet,test.lbs$y)
performance_statistics(nnet.lbs)
end.time <- Sys.time()
nnet.lbs.time <- end.time-start.time

```


# Random Forest 
```{r}
# Random Forest
start.time <- Sys.time()
y.index<- which(colnames(train.lbs)=="y")
bestmtry <- tuneRF(y= train.lbs$y, x= train.lbs[,-y.index], stepFactor=1.5, improve=1e-5, ntree=600)
best.mtry <- bestmtry[,1][which.min(bestmtry[,2])]
model.rf <- randomForest(y ~ ., data = train.lbs, ntree=600, mtry=best.mtry, importance=T)
pre.rf <- predict(model.rf,test.lbs.x,type="class")
rf.lbs <- table(pre.rf,test.lbs$y)
performance_statistics(rf.lbs)
end.time <- Sys.time()
rf.lbs.time <- end.time-start.time
```


```{r}

```

```{r}

```

```{r}

```

```{r}

```

