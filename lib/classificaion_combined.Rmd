---
title: "R Notebook"
output: html_notebook
---

 
```{r}
packages.used=c("gbm", "caret","DMwR" ,"nnet","randomForest","e1071")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
```

# Load and Process Data 
```{r}
setwd("~/Spr2017-proj5-grp15/data/observation")
short_after_selected <- read.csv("short_after20.csv")
short_after_selected$X <- ifelse(short_after_selected$RETURN >0, 1,0)
short_after_selected <- na.omit(short_after_selected)
short_after_selected <- short_after_selected[,-1]
short_after_selected <- short_after_selected[,-11]
test.index <- sample(1:1500,300,replace = F)
test.sas <- short_after_selected[test.index,]

test.sas.x <- test.sas[,-1]
train.sas <- short_after_selected[-test.index,]


colnames(train.sas)[1] <-"y"
colnames(test.sas)[1] <-"y"

train.sas$y <- as.factor(train.sas$y)
```

# SVM
```{r}
# SVM
model.svm <- svm(y ~ ., data = train.sas, cost = 64, gamma = 0.4)
pre.svm <- predict(model.svm,test.sas.x,type="class")
table(pre.svm,test.sas$y)
```

# Tunning SVM parameter 
```{r}
svm_tune <- tune(svm, 
                 y~DY+EBITG+EV2EBITDA+M2B+MOMENTUM+PB+PE+PF+PS+days,
                 data=train.sas,
                 kernel="radial", 
                 ranges=list(cost=2^(5:8), 
                             gamma=c(0.4,0.5,0.6)))

print(svm_tune)
plot(svm_tune)
```

# BPNN
```{r}
# netural network
model.nnet <- nnet(y ~ ., data = train.sas, linout = F,
                     size = 10, decay = 0.001, maxit = 200,
                     trace = F, MaxNWts=6000)
pre.nnet <- predict(model.nnet,test.sas.x,type="class")
table(pre.nnet,test.sas$y)

```

# Tunding BPNN
```{r}
tune.nnet(y ~ ., data = train.sas, linout = F,
+           size = 1:10, decay = 0.001:0.1, maxit = 200,
+           trace = F, MaxNWts=6000)
```

# Random Forest 
```{r}
# Random Forest 
y.index<- which(colnames(train.sas)=="y")
bestmtry <- tuneRF(y= train.sas$y, x= train.sas[,-y.index], stepFactor=1.5, improve=1e-5, ntree=600)
best.mtry <- bestmtry[,1][which.min(bestmtry[,2])]
model.rf <- randomForest(y ~ ., data = train.sas, ntree=600, mtry=best.mtry, importance=T)
pre.rf <- predict(model.rf,test.sas.x,type="class")
table(pre.rf,test.sas$y)
```

# GBM
```{r}
# GBM
res_gbm = gbm(y~.,data=train.sas,dist="adaboost",n.tree = 400,shrinkage=0.1)
n=gbm.perf(gbm1)
pred.gbm = predict(res_gbm, test.sas.x, n.tree = 400, type = 'response')
pred.gbm = ifelse(pred.gbm > 0.5, 1, 0)
table(pred.gbm,test.sas$y)

```

# Logistic
```{r}
res_logi = glm(y~.,family = binomial(link="logit"),data=train.sas)
pred.logi = predict(res_logi, test.sas.x, type = 'response')
pred.logi = ifelse(pred.logi > 0.5, 1, 0)
table(pred.logi,test.sas$y)

```



# Majority Vote(Equal Weight)
```{r}
# Majority Vote
pre=(as.numeric(as.character(pre.svm))+as.numeric(as.character(pred.gbm))+as.numeric(as.character(pre.rf)))
pre=ifelse(pre>=2,1,0)
table(pre,test.sas$y)

```


