---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---

# Summary 
#### In this project, ...



## Step 0: Related Papers and Projects 
#### We read more than 4 papers including Post-Earnings Announcement Effect, Momentum and Trend Following in Global Asset Allocation, and etc. Details and links of papers can be found in data/paper/ListofStrategyPapers.xlsx. We then implemented methods and algorithms described in those papers. 


## Step 1: Data Collection And Processing Using Matlab 
####• Data source: Using Bloomberg terminal to get stock related data(including EBIT_growth,Price to book value and 7 other indicators) and 06-17 earning announcement data. Original data can be found in data/original. 
####• Size: 9 files with a total size of 150MB 
####• Data Processing: For each announcement, we need to find the correspoding entry with same sticker and date in 9 indicators files respectively. We also calculate 1-year-Momentum = (Closing Price of Date a / Closing Price of Data(a-252) * 100 ). All the processing process took place in Matlab. Codes can be found at lib/data_process_matlab
####• Result: We got 10 processed CSV files and you can find them at data/observation 

## Step 2: Select the stocks based on their Momentum
####• Data source: data/observation
####• Size: 17 files with a total size of 70MB 
####• Data Processing: Using R to select 1500 observations with highest Momentum and use Python to reorganize data and store them in a proper way. ()
####• Result: 

```{r}
setwd("~/Spr2017-proj5-grp15/data/original/")

### Selected 1500 observations with highest Momentum
mom <- read.csv("../observation/momentum39034.csv",header = FALSE)
n_rows=nrow(mom)

## Long_before_earning.csv and Short_after_earning.csv are written using Python
long_before <- read.csv('../observation_processed/long_before_earning.csv')
short_after <- read.csv('../observation_processed/short_after_earning.csv')
nrow_longb=nrow(long_before)
nrow_shorta=nrow(short_after)

mom_sa=c()
for (i in 1:nrow_shorta){
  inter=max(short_after[i,'MOMENTUM'])
  mom_sa=c(mom_sa,inter)
} 
mom_sa_selected=sort(mom_sa,decreasing=TRUE,index.return=TRUE)
sa_selected=mom_sa_selected$ix[1:1500]
sa_data_selected=short_after[sa_selected,]

mom_lb=c()
for (i in 1:nrow_longb){
  inter=max(long_before[i,'MOMENTUM'])
  mom_lb=c(mom_lb,inter)
} 
mom_lb_selected=sort(mom_lb,decreasing=TRUE,index.return=TRUE)
lb_selected=mom_lb_selected$ix[1:1500]
lb_data_selected=long_before[lb_selected,]

# write.csv(lb_data_selected,'../observation_processed/long_before.csv')
# write.csv(sa_data_selected,'../observation_processed/short_after.csv')
```

```{r,warning=F}
packages.used=c("gbm", "caret","DMwR" ,"nnet","randomForest","e1071")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
# install packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(text2vec, plyr,qlcMatrix, kernlab, knitr)
```

# Load and Process Data 
```{r}
short_after_selected <- sa_data_selected
long_after_selected <- lb_data_selected

colnames(long_after_selected)[1] <- "y"
long_after_selected$y <- ifelse(long_after_selected$RETURN >0, 1, 0)
long_after_selected <- long_after_selected[,c(-2,-12)]

colnames(short_after_selected)[1] <- "y"
short_after_selected$y <- ifelse(short_after_selected$RETURN >0, 1,0)
short_after_selected <- short_after_selected[,c(-2,-12)]

test.index <- sample(1:1500,300,replace = F)

test.sas <- short_after_selected[test.index,]
test.lbs <- long_after_selected[test.index,]
test.sas.x <- test.sas[,-1]
test.lbs.x <- test.lbs[,-1]
train.sas <- short_after_selected[-test.index,]
train.lbs <- long_after_selected[-test.index,]
```
```{r,warning=F}
source("../lib/evaluation_measures.R")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
```


# Short After Model 
### GBM
```{r}
# GBM
start.time <- Sys.time()
res_gbm = train.gbm(train.sas)
pred.gbm = test.gbm(res_gbm,test.sas.x)
sas.gbm.sum = table(pred.gbm,test.sas$y)
end.time <- Sys.time()
gbm.sas.time <- end.time-start.time
perf_sas_gbm <- performance_statistics(sas.gbm.sum)
perf_sas_gbm
```
### SVM
```{r}
# model.svm <- svm(y ~ ., data = train.sas, cost = 256, gamma = 0.3)
# Tune svm
start.time <- Sys.time()
model.svm.sas <- train.svm(train.sas)
pre.svm <- test.svm(model.svm.sas,test.sas.x)
svm.sas <- table(pre.svm,test.sas$y)
end.time <- Sys.time()
svm.sas.time <- end.time-start.time
perf_sas_svm <- performance_statistics(svm.sas)
perf_sas_svm

```


### BPNN
```{r}
# netural network
start.time <- Sys.time()
# model.nnet <- nnet(y ~ ., data = train.sas, linout = F, size = 10, decay = 0.001, maxit = 200, trace = F, MaxNWts=6000)
# Tune bpnn
model.nnet <- train.bp(train.sas)
pre.nnet <- test.bp(model.nnet,test.sas.x)
nnet.sas <- table(pre.nnet,test.sas$y)
end.time <- Sys.time()
nnet.sas.time <- end.time-start.time
perf_sas_nnet <- performance_statistics(nnet.sas)
perf_sas_nnet

```


### Random Forest 
```{r}
# Random Forest
start.time <- Sys.time()
model.rf <- train.rf(train.sas)
pre.rf <- test.rf(model.rf,test.sas.x)
rf.sas <- table(pre.rf,test.sas$y)
end.time <- Sys.time()
rf.sas.time <- end.time-start.time
perf_sas_rf <- performance_statistics(rf.sas)
perf_sas_rf

```



### Logistic
```{r}
start.time <- Sys.time()
res_logi = train.log(train.sas)
pred.logi = test.log(res_logi,test.sas.x)
log.sas <- table(pred.logi,test.sas$y)
end.time <- Sys.time()
log.sas.time <- end.time-start.time
perf_sas_log <- performance_statistics(log.sas)
perf_sas_log

```



### Majority Vote(Equal Weight)
```{r}
# Majority Vote
pre=(as.numeric(as.character(pre.svm))+as.numeric(as.character(pred.gbm))+as.numeric(as.character(pre.rf)))
pre=ifelse(pre>=2,1,0)
mv <- table(pre,test.sas$y)
perf_sas_mv <- performance_statistics(mv)
perf_sas_mv
```

# Long - Before Model 
### SVM
```{r}
# model.svm <- svm(y ~ ., data = train.lbs, cost = 256, gamma = 0.3)
# Tune svm
start.time <- Sys.time()
model.svm.lbs <- train.svm2(train.lbs)
pre.svm <- test.svm(model.svm.lbs,test.lbs.x)
svm.lbs <- table(pre.svm,test.lbs$y)
end.time <- Sys.time()
svm.time <- end.time-start.time
perf_lbs_svm <- performance_statistics(svm.lbs)
perf_lbs_svm
```

### BPNN
```{r}
# netural network
start.time <- Sys.time()
# model.nnet <- nnet(y ~ ., data = train.sas, linout = F, size = 10, decay = 0.001, maxit = 200, trace = F, MaxNWts=6000)
# Tune bpnn
model.nnet <- train.bp(train.lbs)
pre.nnet <- test.bp(model.nnet,test.lbs.x)
nnet.lbs <- table(pre.nnet,test.lbs$y)
end.time <- Sys.time()
nnet.lbs.time <- end.time-start.time
perf_lbs_nnet <- performance_statistics(nnet.lbs)
perf_lbs_nnet

```


### Random Forest 
```{r}
# Random Forest
start.time <- Sys.time()
model.rf <- train.rf(train.lbs)
pre.rf <- test.rf(model.rf,test.lbs.x)
rf.lbs <- table(pre.rf,test.lbs$y)
end.time <- Sys.time()
rf.lbs.time <- end.time-start.time
perf_lbs_rf <- performance_statistics(rf.lbs)
perf_lbs_rf
```

### Logistic
```{r}
start.time <- Sys.time()
res_logi = train.log(train.lbs)
pred.logi = test.log(res_logi,test.lbs.x)
log.lbs <- table(pred.logi,test.lbs$y)
end.time <- Sys.time()
log.lbs.time <- end.time-start.time
perf_lbs_log <- performance_statistics(log.lbs)
perf_lbs_log
```

### GBM
```{r}
# GBM
start.time <- Sys.time()
res_gbm = train.gbm(train.lbs)
pred.gbm = test.gbm(res_gbm,test.lbs.x)
sas.gbm.sum = table(pred.gbm,test.lbs$y)
end.time <- Sys.time()
gbm.sas.time <- end.time-start.time
perf_lbs_gbm <- performance_statistics(sas.gbm.sum)
perf_lbs_gbm
```
### Short-After-Model Summary
```{r}
library(kernlab)
compare_df <- data.frame(method=c("GBM","SVM","NNET","RF","LOGISTIC","MV"),
                         precision=c(perf_sas_gbm$precision,perf_sas_svm$precision,perf_sas_nnet$precision,perf_sas_rf$precision,perf_sas_log$precision,perf_sas_mv$precision),
                         recall=c(perf_sas_gbm$recall,perf_sas_svm$recall,perf_sas_nnet$recall,perf_sas_rf$recall,perf_sas_log$recall,perf_sas_mv$recall),
                         f1=c(perf_sas_gbm$f1,perf_sas_svm$f1,perf_sas_nnet$f1,perf_sas_rf$f1,perf_sas_log$f1,perf_sas_mv$f1),
                         accuracy=c(perf_sas_gbm$accuracy,perf_sas_svm$accuracy,perf_sas_nnet$accuracy,perf_sas_rf$accuracy,perf_sas_log$accuracy,perf_sas_mv$accuracy),
                         time=c(gbm.sas.time,svm.sas.time,nnet.sas.time,rf.sas.time,log.sas.time,"NA"))
kable(compare_df,caption="Comparision of performance for two different methods(Short-After-Model)", digits=2)
```
### Long-Before-Model Summary
```{r}
compare_df <- data.frame(method=c("GBM","SVM","NNET","RF","LOGISTIC"),
                         precision=c(perf_lbs_gbm$precision,perf_lbs_svm$precision,perf_lbs_nnet$precision,perf_lbs_rf$precision,perf_lbs_log$precision),
                         recall=c(perf_lbs_gbm$recall,perf_lbs_svm$recall,perf_lbs_nnet$recall,perf_lbs_rf$recall,perf_lbs_log$recall),
                         f1=c(perf_lbs_gbm$f1,perf_lbs_svm$f1,perf_lbs_nnet$f1,perf_lbs_rf$f1,perf_lbs_log$f1),                     accuracy=c(perf_lbs_gbm$accuracy,perf_lbs_svm$accuracy,perf_lbs_nnet$accuracy,perf_lbs_rf$accuracy,perf_lbs_log$accuracy),
                         time=c(gbm.lbs.time,svm.lbs.time,nnet.lbs.time,rf.lbs.time,log.lbs.time))
                         
kable(compare_df,caption="Comparision of performance for two different methods(Long-Before-Model)", digits=2)
```

