---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---

# Summary 
#### In this project, we combined core value of 8 published articles related to Earnings Release and improve it with machine learning strategy.  Finally, we got two models: long-before earning release and short-after earning release. Our model could tell you, if you long the stock before earning announcement, whether you could earn a positive return. If you short the stock after earning announcement, whether you could get a positive return.



## Step 0: Related Papers and Projects 
#### We read more than 8 papers including Post-Earnings Announcement Effect, Momentum and Trend Following in Global Asset Allocation, and etc. Details and links of papers can be found at data/paper/ListofStrategyPapers.xlsx. We then implemented methods and algorithms described in those papers. 


## Step 1: Data Collection And Processing
### Part a: Using Matlab to process data
####• Data source: Using Bloomberg terminal to get stock related data(including EBIT_growth,Price to book value and 7 other indicators) and 06-17 earning announcement data. Original data can be found in data/original. 
####• Size: 9 files with a total size of 150MB 
####• Data Processing: For each announcement, we need to find the correspoding entry with same sticker and date in 9 indicators files respectively. We also calculate 1-year-Momentum = (Closing Price of Date a / Closing Price of Data(a-252) * 100 ). All the processing process took place in Matlab. Codes can be found at lib/data_process_matlab
####• Result: We got 10 processed CSV files and you can find them at data/observation 


### Part b: Using R to run regression and find the most important indicators 
####• Data source: data/observation_processed/
####• Size: 4 files 
####• Data Processing: Using StepWise AIC in R 
####• Result: Momentum and PS are two signiciant indicators

#### Load library
```{r}
library(DAAG)
library(MASS)
library(leaps)
library(car)
```

#### Process Data
```{r}
setwd("~/Spr2017-proj5-grp15/data/original/")
longBefore <- read.csv("../observation_processed/long_before_earning.csv")
longAfter <- read.csv("../observation_processed/long_after_earning.csv")
shortBefore <- read.csv("../observation_processed/short_before_earning.csv")
shortAfter <- read.csv("../observation_processed/short_after_earning.csv")
```
##### Using stepwise AIC to find the most inportant indicators 
```{r,message=F}
longBefore <- longBefore[,-1]
longAfter <- longAfter[,-1]
shortAfter <- shortAfter[,-1]
shortBefore <- shortBefore[,-1]
fitLongAfter <- lm(RETURN~DY+EBITG+EV2EBITDA+M2B+MOMENTUM+PB+PE+PF+PS+days+surprise,data=longAfter)
stepLA <- stepAIC(fitLongAfter, direction="both")
summary(stepLA)
fitLongBefore <- lm(RETURN~DY+EBITG+EV2EBITDA+M2B+MOMENTUM+PB+PE+PF+PS+days,data=longBefore)
stepLB <- stepAIC(fitLongBefore, direction="both")
summary(stepLB)
fitShortBefore <- lm(RETURN~DY+EBITG+EV2EBITDA+M2B+MOMENTUM+PB+PE+PF+PS+days,data=shortBefore)
stepSB <- stepAIC(fitShortBefore, direction="both")
summary(stepSB)
fitShortAfter <- lm(RETURN~DY+EBITG+EV2EBITDA+M2B+MOMENTUM+PB+PE+PF+PS+days,data=shortAfter)
stepSA <- stepAIC(fitShortAfter, direction="both")
summary(stepSA)
```


## Step 2: Select the stocks based on their Momentum
####• Data source: data/observation
####• Size: 17 files with a total size of 70MB 
####• Data Processing: Using R to select 1500 observations with highest Momentum and use Python to reorganize data and store them in a proper way. Python code can be found at lib/reorganized.py. 
####• Result: We got 2 models: Long-Before-Model and Short-After-Model
```{r}
### Selected 1500 observations with highest Momentum
setwd("~/Spr2017-proj5-grp15/data/original/")
mom <- read.csv("../observation/momentum39034.csv",header = FALSE)
n_rows=nrow(mom)

## Long_before_earning.csv and Short_after_earning.csv are written using Python
long_before <- read.csv('../observation_processed/long_before_earning.csv')
short_after <- read.csv('../observation_processed/short_after_earning.csv')
nrow_longb=nrow(long_before)
nrow_shorta=nrow(short_after)

mom_sa=c()
for (i in 1:nrow_shorta){
  inter=max(short_after[i,'MOMENTUM'])
  mom_sa=c(mom_sa,inter)
} 
mom_sa_selected=sort(mom_sa,decreasing=TRUE,index.return=TRUE)
sa_selected=mom_sa_selected$ix[1:1500]
sa_data_selected=short_after[sa_selected,]

mom_lb=c()
for (i in 1:nrow_longb){
  inter=max(long_before[i,'MOMENTUM'])
  mom_lb=c(mom_lb,inter)
} 
mom_lb_selected=sort(mom_lb,decreasing=TRUE,index.return=TRUE)
lb_selected=mom_lb_selected$ix[1:1500]
lb_data_selected=long_before[lb_selected,]

# write.csv(lb_data_selected,'../observation_processed/long_before.csv')
# write.csv(sa_data_selected,'../observation_processed/short_after.csv')
```


## Step 3: Classification model 
####• Data source: data/obesrvation_processed
####• Size: 2 files with a total size of 140kb
####• Data Processing: Devidide data into training set(1200 observations) and test set(300 obervations) 
####• Classification method: GBM, SVM, NNET, Random Forest, and Logistic. We also taken into account majority vote(GBM,SVM and Random Forest) 
####• Result: as following codes will show 
```{r,warning=F}
packages.used=c("gbm", "caret","DMwR" ,"nnet","randomForest","e1071")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
# install packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(text2vec, plyr,qlcMatrix, kernlab, knitr)
```

# Load and Process Data 
```{r}
short_after_selected <- sa_data_selected
long_after_selected <- lb_data_selected

colnames(long_after_selected)[1] <- "y"
long_after_selected$y <- ifelse(long_after_selected$RETURN >0, 1, 0)
long_after_selected <- long_after_selected[,-11]

colnames(short_after_selected)[1] <- "y"
short_after_selected$y <- ifelse(short_after_selected$RETURN >0, 1,0)
short_after_selected <- short_after_selected[,-11]

test.index <- sample(1:1500,300,replace = F)

test.sas <- short_after_selected[test.index,]
test.lbs <- long_after_selected[test.index,]
test.sas.x <- test.sas[,-1]
test.lbs.x <- test.lbs[,-1]
train.sas <- short_after_selected[-test.index,]
train.lbs <- long_after_selected[-test.index,]
```

#### Load requred functions 
```{r,warning=F,message=F}
source("../lib/evaluation_measures.R")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
```


# Short After Model 
### GBM
```{r}
# GBM
start.time <- Sys.time()
res_gbm = train.gbm(train.sas)
pred.gbm = test.gbm(res_gbm,test.sas.x)
sas.gbm.sum = table(pred.gbm,test.sas$y)
end.time <- Sys.time()
gbm.sas.time <- end.time-start.time
perf_sas_gbm <- performance_statistics(sas.gbm.sum)
perf_sas_gbm
```
### SVM
```{r}
# model.svm <- svm(y ~ ., data = train.sas, cost = 256, gamma = 0.3)
# Tune svm
start.time <- Sys.time()
model.svm.sas <- train.svm(train.sas)
pre.svm <- test.svm(model.svm.sas,test.sas.x)
svm.sas <- table(pre.svm,test.sas$y)
end.time <- Sys.time()
svm.sas.time <- end.time-start.time
perf_sas_svm <- performance_statistics(svm.sas)
perf_sas_svm

```


### BPNN
```{r}
# netural network
start.time <- Sys.time()
# model.nnet <- nnet(y ~ ., data = train.sas, linout = F, size = 10, decay = 0.001, maxit = 200, trace = F, MaxNWts=6000)
# Tune bpnn
model.nnet <- train.bp(train.sas)
pre.nnet <- test.bp(model.nnet,test.sas.x)
nnet.sas <- table(pre.nnet,test.sas$y)
end.time <- Sys.time()
nnet.sas.time <- end.time-start.time
perf_sas_nnet <- performance_statistics(nnet.sas)
perf_sas_nnet

```


### Random Forest 
```{r}
# Random Forest
start.time <- Sys.time()
model.rf <- train.rf(train.sas)
pre.rf <- test.rf(model.rf,test.sas.x)
rf.sas <- table(pre.rf,test.sas$y)
end.time <- Sys.time()
rf.sas.time <- end.time-start.time
perf_sas_rf <- performance_statistics(rf.sas)
perf_sas_rf

```



### Logistic
```{r}
start.time <- Sys.time()
res_logi = train.log(train.sas)
pred.logi = test.log(res_logi,test.sas.x)
log.sas <- table(pred.logi,test.sas$y)
end.time <- Sys.time()
log.sas.time <- end.time-start.time
perf_sas_log <- performance_statistics(log.sas)
perf_sas_log

```



### Majority Vote(Equal Weight)
```{r}
# Majority Vote
pre=(as.numeric(as.character(pre.svm))+as.numeric(as.character(pred.gbm))+as.numeric(as.character(pre.rf)))
pre=ifelse(pre>=2,1,0)
mv <- table(pre,test.sas$y)
perf_sas_mv <- performance_statistics(mv)
perf_sas_mv
```

# Long - Before Model 
### SVM
```{r}
# model.svm <- svm(y ~ ., data = train.lbs, cost = 256, gamma = 0.3)
# Tune svm
start.time <- Sys.time()
model.svm.lbs <- train.svm(train.lbs)
pre.svm <- test.svm(model.svm.lbs,test.lbs.x)
svm.lbs <- table(pre.svm,test.lbs$y)
end.time <- Sys.time()
svm.lbs.time <- end.time-start.time
perf_lbs_svm <- performance_statistics(svm.lbs)
perf_lbs_svm
```

### BPNN
```{r}
# netural network
start.time <- Sys.time()
# model.nnet <- nnet(y ~ ., data = train.sas, linout = F, size = 10, decay = 0.001, maxit = 200, trace = F, MaxNWts=6000)
# Tune bpnn
model.nnet <- train.bp(train.lbs)
pre.nnet <- test.bp(model.nnet,test.lbs.x)
nnet.lbs <- table(pre.nnet,test.lbs$y)
end.time <- Sys.time()
nnet.lbs.time <- end.time-start.time
perf_lbs_nnet <- performance_statistics(nnet.lbs)
perf_lbs_nnet
```


### Random Forest 
```{r}
# Random Forest
start.time <- Sys.time()
model.rf <- train.rf(train.lbs)
pre.rf <- test.rf(model.rf,test.lbs.x)
rf.lbs <- table(pre.rf,test.lbs$y)
end.time <- Sys.time()
rf.lbs.time <- end.time-start.time
perf_lbs_rf <- performance_statistics(rf.lbs)
perf_lbs_rf
```

### Logistic
```{r}
start.time <- Sys.time()
res_logi = train.log(train.lbs)
pred.logi = test.log(res_logi,test.lbs.x)
log.lbs <- table(pred.logi,test.lbs$y)
end.time <- Sys.time()
log.lbs.time <- end.time-start.time
perf_lbs_log <- performance_statistics(log.lbs)
perf_lbs_log
```

### GBM
```{r}
# GBM
start.time <- Sys.time()

res_gbm = train.gbm(train.lbs)
pred.gbm = test.gbm(res_gbm,test.lbs.x)
lbs.gbm.sum = table(pred.gbm,test.lbs$y)

end.time <- Sys.time()
gbm.lbs.time <- end.time-start.time

perf_lbs_gbm <- performance_statistics(lbs.gbm.sum)
perf_lbs_gbm
```

### Short-After-Model Summary
```{r}
compare_df <- data.frame(method=c("GBM","SVM","NNET","RF","LOGISTIC","MV"),
                         precision=c(perf_sas_gbm$precision,perf_sas_svm$precision,perf_sas_nnet$precision,perf_sas_rf$precision,perf_sas_log$precision,perf_sas_mv$precision),
                         recall=c(perf_sas_gbm$recall,perf_sas_svm$recall,perf_sas_nnet$recall,perf_sas_rf$recall,perf_sas_log$recall,perf_sas_mv$recall),
                         f1=c(perf_sas_gbm$f1,perf_sas_svm$f1,perf_sas_nnet$f1,perf_sas_rf$f1,perf_sas_log$f1,perf_sas_mv$f1),
                         accuracy=c(perf_sas_gbm$accuracy,perf_sas_svm$accuracy,perf_sas_nnet$accuracy,perf_sas_rf$accuracy,perf_sas_log$accuracy,perf_sas_mv$accuracy),
                         time=c(gbm.sas.time,svm.sas.time,nnet.sas.time,rf.sas.time,log.sas.time,"NA"))
kable(compare_df,caption="Comparision of performance for different methods(Short-After-Model)", digits=2)
```

### Long-Before-Model Summary
```{r}
compare_df <- data.frame(method=c("GBM","SVM","NNET","RF","LOGISTIC"),
                         precision=c(perf_lbs_gbm$precision,perf_lbs_svm$precision,perf_lbs_nnet$precision,perf_lbs_rf$precision,perf_lbs_log$precision),
                         recall=c(perf_lbs_gbm$recall,perf_lbs_svm$recall,perf_lbs_nnet$recall,perf_lbs_rf$recall,perf_lbs_log$recall),
                         f1=c(perf_lbs_gbm$f1,perf_lbs_svm$f1,perf_lbs_nnet$f1,perf_lbs_rf$f1,perf_lbs_log$f1),                      accuracy=c(perf_lbs_gbm$accuracy,perf_lbs_svm$accuracy,perf_lbs_nnet$accuracy,perf_lbs_rf$accuracy,perf_lbs_log$accuracy),
                         time=c(gbm.lbs.time,svm.lbs.time,nnet.lbs.time,rf.lbs.time,log.lbs.time))
                         
kable(compare_df,caption="Comparision of performance for different methods(Long-Before-Model)", digits=2)
```


## Step 4: Including news features from NLP
### Part a: Get Data 
####• Data source: Bloomberg and WSJ news and headlines
####• Purpose: Web scrapying using Python from WSJ and Bloomberg. You can find the code at lib/wsj_news_scrape_python/run.py and extract_news.ipynb. 
####• Data Processing: Using scrapy and python package "selenium"
####• Result: WSJ news stored at data/wsjnews and Bloomberg headlines stored at data/news_Bloomberg


### Part b: NLP 
####• Data source: from previous step
####• Purpose: We would like to analyze if these articles from mainstream media could influence stock market and investor’s trading strategie
####• Data Processing: Extracting features using NLP. Specifically, we use R built-in pacakge "syuzhetand" and word_count. 
####• Result: As follows
```{r}
# source("../lib/sentimental.r")
```

### Part c: Used R to match and clean news data
####• Data source: From previous steps
####• Purpose: For every news data we found, we use R to find the corresponding indicators(previous financial features) with the same date and ticker. We reorganized them and save them as RData file. 
####• Data Processing: We use R, and you can find the code at lib/features_matching.R
####• Result: RData file for futher processing. 
```{r}
# source("../lib/features_matching.r")
```

## Step 5: 
####• Data source: data/all_features.csv
####• Purpose: Do classification and model evluation based on new features extracted from news headlines 
####• Data Processing: Using R to run classification method including GBM, SVM, NNET, Random Forest, and Logistic as in step 3.
####• Result: As below 
```{r}
setwd("~/Spr2017-proj5-grp15/data/original/")
all.features <- read.csv("../all_features.csv")
```

### Data Process
```{r}
all.features[is.na(all.features)] <- 0
all.features$X <- all.features$y.return01
all.features <- all.features[,-ncol(all.features)]
colnames(all.features)[1] <- "y"
all.features <- all.features[,-c(2,3)]

n <- nrow(all.features)

test.ind <- sample(1:n,n/3,replace = F)

test.feature <- all.features[test.ind,]
test.feature.x <- test.feature[,-1]
train.feature <- all.features[-test.ind,]
```

### GBM
```{r}
# GBM
start.time <- Sys.time()
res_gbm = train.gbm(train.feature)
pred.gbm = test.gbm(res_gbm,test.feature.x)
feature.gbm.sum = table(pred.gbm,test.feature$y)
end.time <- Sys.time()
gbm.feature.time <- end.time-start.time
perf_feature_gbm <- performance_statistics(feature.gbm.sum)
perf_feature_gbm
```

### SVM
```{r}
# model.svm <- svm(y ~ ., data = train.sas, cost = 256, gamma = 0.3)
# Tune svm
start.time <- Sys.time()
model.svm.feature <- train.svm(train.feature)
pre.svm <- test.svm(model.svm.feature,test.feature.x)
svm.feature <- table(pre.svm,test.feature$y)
end.time <- Sys.time()
svm.feature.time <- end.time-start.time
perf_feature_svm <- performance_statistics(svm.feature)
perf_feature_svm

```

### NNET
```{r}
# netural network
start.time <- Sys.time()
# model.nnet <- nnet(y ~ ., data = train.sas, linout = F, size = 10, decay = 0.001, maxit = 200, trace = F, MaxNWts=6000)
# Tune bpnn
model.nnet <- train.bp(train.feature)
pre.nnet <- test.bp(model.nnet,test.feature.x)
nnet.feature <- table(pre.nnet,test.feature$y)
end.time <- Sys.time()
nnet.feature.time <- end.time-start.time
perf_feature_nnet <- performance_statistics(nnet.feature)
perf_feature_nnet
```

### Random Forest 
```{r}
# Random Forest
start.time <- Sys.time()
model.rf <- train.rf(train.feature)
pre.rf <- test.rf(model.rf,test.feature.x)
rf.feature <- table(pre.rf,test.feature$y)
end.time <- Sys.time()
rf.feature.time <- end.time-start.time
perf_feature_rf <- performance_statistics(rf.feature)
perf_feature_rf

```

### Logistic
```{r}
start.time <- Sys.time()
res_logi = train.log(train.feature)
pred.logi = test.log(res_logi,test.feature.x)
log.feature <- table(pred.logi,test.feature$y)
end.time <- Sys.time()
log.feature.time <- end.time-start.time
perf_feature_log <- performance_statistics(log.feature)
perf_feature_log
```

### Model Summary
```{r}
compare_df <- data.frame(method=c("GBM","SVM","NNET","RF","LOGISTIC"),
                         precision=c(perf_feature_gbm$precision,perf_feature_svm$precision,perf_feature_nnet$precision,perf_feature_rf$precision,perf_feature_log$precision),
                         recall=c(perf_feature_gbm$recall,perf_feature_svm$recall,perf_feature_nnet$recall,perf_feature_rf$recall,perf_feature_log$recall),
                         f1=c(perf_feature_gbm$f1,perf_feature_svm$f1,perf_feature_nnet$f1,perf_feature_rf$f1,perf_feature_log$f1),                      accuracy=c(perf_feature_gbm$accuracy,perf_feature_svm$accuracy,perf_feature_nnet$accuracy,perf_feature_rf$accuracy,perf_feature_log$accuracy),
                         time=c(gbm.feature.time,svm.feature.time,nnet.feature.time,rf.feature.time,log.feature.time))
                         
kable(compare_df,caption="Comparision of performance for different methods", digits=2)
```

### Two Models Comparison 
```{r}
compare_df <- data.frame(model=c("Short-After-Model","NLP"),
                         precision=c(perf_sas_rf$precision,perf_feature_rf$precision),
                         recall=c(perf_sas_rf$recall,perf_feature_rf$recall),
                         f1=c(perf_sas_rf$f1,perf_feature_rf$f1),                    accuracy=c(perf_sas_rf$accuracy,perf_feature_rf$accuracy),
                         time=c(rf.sas.time,rf.feature.time))
                         
kable(compare_df,caption="Comparision of performance for two different models", digits=2)
```

# Conclusion
##• Stock market is known as a chaotic system. We get more information on news by limiting our scope to earning release day, and are able to build the prediction model of more than 85% accuracy. However, with the help of NLP our model does not improve but worse off. We may need to reconsider the weight of each features and do better feature selection. 
##• Models and features: GBM, SVM, NNET, RF, Logistic, and Random forest. Random forest gives us the best among all of them
##• Further, it is observed that data visualization from different companies shows that data set from a certain company is much more distinguishable than mixing data (portfolio and index)
##• Compared with our goal, we just made it. We may check whether the strategy still works with recent 10 years data, improve the model with statistical technique and redict whether to long or short before & after Earnings Release and so on. 
##• We use their core value: long before Earnings Release, Short after Earnings Release and use machine learning model to improve it
##• NLP analysis: The sentiment result from toolkit is too general to apply for financial news and our word vector list is quite limited
##• Model Improvement: high dimension can be handled better
  
